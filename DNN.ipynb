{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "261325ba",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks\n",
    "TensorFlow operations (also called ops for short) can take any number of inputs and\n",
    "produce any number of outputs (they are called source ops). ANNs are at the very core of Deep Learning. They are versatile, powerful, and scalable, making them ideal to tackle large and highly complex Machine Learning tasks\n",
    "### Linear threshold unit (LTU)\n",
    "The LTU computes a weighted sum of its inputs, then applies a step function to that sum and outputs the result. A single LTU can be used for simple linear binary classification. It computes a linear\n",
    "combination of the inputs and if the result exceeds a threshold, it outputs the positive\n",
    "class or else outputs the negative class (just like a Logistic Regression classifier or a\n",
    "linear SVM). if the\n",
    "training instances are linearly separable, Rosenblatt demonstrated that this algorithm\n",
    "would converge to a solution. 7 This is called the Perceptron convergence theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37b8386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perimport tensorflow as tf\n",
    "\n",
    "dnn_clf = tf.estimator.DNNClassifier(hidden_units=[300, 100], n_classes=10, feature_columns=['lenght','width'])\n",
    "dnn_clf.fit(x=X_train, y=y_train, batch_size=50, steps=40000)ceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4f63b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # petal length, petal width\n",
    "y = (iris.target == 0).astype(int) # Iris Setosa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b74767d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0bea2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(random_state=42)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f2e3b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea08d1bd",
   "metadata": {},
   "source": [
    "In fact, Scikit-Learn’s Perceptron class is equivalent to\n",
    "using an SGDClassifier with the following hyperparameters: loss=\"perceptron\" ,\n",
    "learning_rate=\"constant\" , eta0=1 (the learning rate), and penalty=None (no regu‐\n",
    "larization).\n",
    "\n",
    "Perceptrons do not output a class\n",
    "probability; rather, they just make predictions based on a hard threshold. This is one\n",
    "of the good reasons to prefer Logistic Regression over Perceptrons.\n",
    "\n",
    "the fact that they are incapable of solving some trivial problems (e.g., the Exclusive OR (XOR)\n",
    "classification problem. Of course this is true of any other linear classification model as well. some of the limitations of Perceptrons can be eliminated by stacking multiple Perceptrons. The resulting ANN is called a Multi-Layer Perceptron (MLP). When an ANN has two or more hidden layers, it is called\n",
    "a deep neural network (DNN) \n",
    "\n",
    "### Training an MLP with TensorFlow’s High-Level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79ca6ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f675a063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7f0da0bc9dd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Dense(10, activation='relu'),\n",
    "                             tf.keras.layers.Dense(10, activation='relu'),\n",
    "                             tf.keras.layers.Dense(3, activation='softmax')])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a83de8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8893d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=50, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2431194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d30cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = np.argmax(y_test,axis=1)\n",
    "predicted = np.argmax(y_pred,axis=1)\n",
    "print(f\"Actual: {actual}\")\n",
    "print(f\"Predicted: {predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6ae79d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = list(dnn_clf.predict(X_test))\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b6c6af",
   "metadata": {},
   "source": [
    "If you run this code on the MNIST dataset (after scaling it, e.g., by using Scikit-\n",
    "Learn’s StandardScaler ), you may actually get a model that achieves over 98.1%\n",
    "accuracy on the test set!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef61afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_clf.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
