{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1567909",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "Reducing dimensionality does lose some information (just like compressing an image to JPEG can degrade its quality), so even though it will speed up training, it may also make your system perform slightly worse. It also makes your pipelines a bit more complex and thus harder to maintain. So you should first try to train your system with the original data before considering using dimensionality reduction if training is too slow. In some cases, however, reducing the dimensionality of the training data may filter out some noise and unnecessary details and thus result in higher performance (but in general it won’t; it will just speed up training).\n",
    "\n",
    "Apart from speeding up training, it is also extremely useful\n",
    "for data visualization (or DataViz). Reducing the number of dimensions down to two\n",
    "205(or three) makes it possible to plot a high-dimensional training set on a graph and\n",
    "often gain some important insights by visually detecting patterns, such as clusters. we will present 2 main approaches to dimensionality reduction (projection and Manifold Learning), and we will go\n",
    "through three of the most popular dimensionality reduction techniques: PCA, Kernel PCA, and LLE.\n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "It is by far the most popular dimensionality reduc‐\n",
    "tion algorithm. First it identifies the hyperplane that lies closest to the data, and then\n",
    "it projects the data onto it. PCA identifies the axis that accounts for the largest amount of variance in the training set as solid line, then 2nd and so on..., it's called i th principal component (PC) c1...\n",
    "\n",
    "The direction of the principal components is not stable: if you per‐\n",
    "turb the training set slightly and run PCA again, some of the new\n",
    "PCs may point in the opposite direction of the original PCs. How‐\n",
    "ever, they will generally still lie on the same axes. In some cases, a\n",
    "pair of PCs may even rotate or swap, but the plane they define will\n",
    "generally remain the same. if you implement PCA yourself (as in the pre‐\n",
    "ceding example), or if you use other libraries, don’t forget to center\n",
    "the data first.\n",
    "\n",
    "### Projecting Down to d Dimensions\n",
    "Once you have identified all the principal components, you can reduce the dimen‐\n",
    "sionality of the dataset down to d dimensions by projecting it onto the hyperplane\n",
    "defined by the first d principal components. You now know how to reduce the dimensionality of any dataset\n",
    "down to any number of dimensions, while preserving as much variance as possible.\n",
    "\n",
    "Scikit-Learn’s PCA class implements PCA using SVD decomposition, (note that it automatically takes care of centering the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cdbc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d1546",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.T[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00327db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the explained variance ratio of each principal component\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5713afd1",
   "metadata": {},
   "source": [
    "array([ 0.84248607, 0.14631839]), This tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6% lies along the second axis. This leaves less than 1.2% for the third axis, so it is reasonable to assume that it probably carries little information.\n",
    "\n",
    "### Choosing the Right Number of Dimensions\n",
    "it is\n",
    "generally preferable to choose the number of dimensions that add up to a sufficiently\n",
    "large portion of the variance (e.g., 95%). Unless, of course, you are reducing dimen‐\n",
    "sionality for data visualization—in that case you will generally want to reduce the\n",
    "dimensionality down to 2 or 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be1db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef58f760",
   "metadata": {},
   "source": [
    "### PCA for Compression\n",
    "the reconstructed data\n",
    "(compressed and then decompressed) is called the reconstruction error. For example,\n",
    "the following code compresses the MNIST dataset down to 154 dimensions, then uses\n",
    "the inverse_transform() method to decompress it back to 784 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea6abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 154)\n",
    "X_mnist_reduced = pca.fit_transform(X_mnist)\n",
    "X_mnist_recovered = pca.inverse_transform(X_mnist_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9486ac0",
   "metadata": {},
   "source": [
    "### Incremental PCA\n",
    "the preceding implementation of PCA is that it requires the whole\n",
    "training set to fit in memory in order for the SVD algorithm to run. Fortunately,\n",
    "Incremental PCA (IPCA) algorithms have been developed: you can split the training\n",
    "set into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is\n",
    "useful for large training sets, and also to apply PCA online (i.e., on the fly, as new\n",
    "instances arrive).\n",
    "\n",
    "The following code splits the MNIST dataset into 100 mini-batches (using NumPy’s\n",
    "array_split() function) and feeds them to Scikit-Learn’s IncrementalPCA class 5 to\n",
    "reduce the dimensionality of the MNIST dataset down to 154 dimensions (just like\n",
    "before). Note that you must call the partial_fit() method with each mini-batch\n",
    "rather than the fit() method with the whole training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f23acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "\n",
    "for X_batch in np.array_split(X_mnist, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "    \n",
    "X_mnist_reduced = inc_pca.transform(X_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3359062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively\n",
    "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\n",
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3da6f6",
   "metadata": {},
   "source": [
    "### Randomized PCA\n",
    "Scikit-Learn offers yet another option to perform PCA, called Randomized PCA. This\n",
    "is a stochastic algorithm that quickly finds an approximation of the first d principal\n",
    "components. it is dramatically faster than the previous algorithms when d is much\n",
    "smaller than n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c1eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d212f47",
   "metadata": {},
   "source": [
    "### Kernel PCA\n",
    "the kernel trick can be applied to PCA, making it possible to perform\n",
    "complex nonlinear projections for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0a7f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261736e4",
   "metadata": {},
   "source": [
    "### Selecting a Kernel and Tuning Hyperparameters\n",
    "dimensionality reduction is often a preparation step for a supervised learning task\n",
    "(e.g., classification), so you can simply use grid search to select the kernel and hyper‐\n",
    "parameters that lead to the best performance on that task.\n",
    "\n",
    "the following code creates a two-step pipeline, first reducing dimensionality to two dimensions\n",
    "using kPCA, then applying Logistic Regression for classification. Then it uses Grid\n",
    "SearchCV to find the best kernel and gamma value for kPCA in order to get the best\n",
    "classification accuracy at the end of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a96b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96445d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([(\"kpca\", KernelPCA(n_components=2)),(\"log_reg\", LogisticRegression())])\n",
    "\n",
    "param_grid = [{\"kpca__gamma\": np.linspace(0.03, 0.05, 10),\"kpca__kernel\": [\"rbf\", \"sigmoid\"]}]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7364f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4bcbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5830ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can then compute the reconstruction pre-image error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(X, X_preimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00450e74",
   "metadata": {},
   "source": [
    "Now you can use grid search with cross-validation to find the kernel and hyperpara‐\n",
    "meters that minimize this pre-image reconstruction error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf48fd",
   "metadata": {},
   "source": [
    "### Locally Linear Embedding (LLE)\n",
    "another very powerful nonlinear dimensionality\n",
    "reduction (NLDR) technique. It is a Manifold Learning technique that does not rely\n",
    "on projections like the previous algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "X_reduced = lle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c68211f",
   "metadata": {},
   "source": [
    "### Other Dimensionality Reduction Techniques\n",
    "* Multidimensional Scaling (MDS) \n",
    "* Isomap\n",
    "* t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "* Linear Discriminant Analysis (LDA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
